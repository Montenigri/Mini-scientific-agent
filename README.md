# Mini-scientific-agent

# ğŸ§  Agente RAG Locale con Ollama, LangChain & Tavily

Un agente AI minimale ma completo che combina **LLM locali**, **retrieval-augmented generation (RAG)** e **fallback su motore di ricerca**, progettato per girare in modo efficiente anche su hardware modesto.

Lâ€™obiettivo di questo progetto Ã¨:

* Imparare a costruire un agente AI passo dopo passo.
* Esplorare lâ€™integrazione di diversi componenti di LangChain.
* Mantenere il tutto leggero abbastanza da girare su un portatile con **8GB di RAM e CPU only**.
* Preparare un ambiente portabile (tramite **Docker**).

---

## âœ¨ FunzionalitÃ 

* ğŸ”¹ **Inferenza LLM locale** tramite [Ollama](https://ollama.ai/) (supporto a modelli piccoli/quantizzati come `phi`, `mistral`, `llama2`).
* ğŸ”¹ **Pipeline RAG**: caricamento di PDF, suddivisione in chunk e memorizzazione in **Chroma** come vector DB.
* ğŸ”¹ **Ricerca web** tramite [Tavily](https://python.langchain.com/docs/integrations/tools/tavily), usata come fallback se il RAG non ha informazioni rilevanti.
* ğŸ”¹ **Agente ReAct** basato su LangGraph, capace di decidere quando usare RAG o il motore di ricerca.

---

## ğŸ“‚ Struttura del progetto

* `pdfs/` â†’ cartella per i documenti da indicizzare.
* `chroma_langchain_db/` â†’ database persistente con gli embeddings.
* `main.py` â†’ entrypoint del progetto, inizializza modelli, DB, strumenti e agente.
* `.env` â†’ variabili dâ€™ambiente (modelli Ollama, path del DB, ecc.).

---

## âœ… Stato del progetto

* [x] Setup ambiente e variabili dâ€™ambiente (.env)
* [x] Integrazione con Ollama per LLM ed embeddings
* [x] Creazione e persistenza di un Vector Store (Chroma)
* [x] Caricamento e split di documenti PDF
* [x] Implementazione del RAG retriever come tool
* [x] Integrazione di un motore di ricerca (Tavily) come fallback
* [x] Creazione dellâ€™agente ReAct con LangGraph
* [ ] Creare unâ€™interfaccia web minimale
* [ ] Internazionalizzazione completa (IT/EN)
* [ ] Containerizzazione con Docker
* [ ] Ottimizzazione prestazioni e logging avanzato

---

## ğŸš€ Requisiti

* Python 3.10+
* [Ollama](https://ollama.ai/) installato e in esecuzione in locale
* Tavily API key (gratuita, da inserire nel `.env`)
* Librerie Python (vedi `requirements.txt`)

---

## â–¶ï¸ Avvio

1. Clona la repo
2. Copia il file `.env.example`, rinominarlo in `.env` ed inserire i parametri necessari (es. modello Ollama, path DB, API key)
3. Inserisci i PDF in `./pdfs`
4. Avvia lo script:

```bash
python main.py
```

---

## ğŸ”œ Prossimi sviluppi

* Creare una **UI web minimale** (Flask/FastAPI + HTMX/VanillaJS).
* Packaging in Docker per facile distribuzione.

---
